{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import corpus here\n",
    "# nltk.download() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I chose to only use 5 categories in my work here because I did not want overload my kernel as I am on a MacBookAir and do not have unlimited space in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_adventure = brown.words(categories='adventure')\n",
    "raw_lore = brown.words(categories='lore')\n",
    "raw_mystery = brown.words(categories='mystery')\n",
    "raw_romance = brown.words(categories='romance')\n",
    "raw_science_fiction = brown.words(categories='science_fiction')\n",
    "\n",
    "raw_adventure = str(raw_adventure)\n",
    "raw_lore = str(raw_lore)\n",
    "raw_mystery = str(raw_mystery)\n",
    "raw_romance = str(raw_romance)\n",
    "raw_science_fiction = str(raw_science_fiction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub('[\\',]',' ',text)\n",
    "    text = re.sub(\"[\\[]*?![\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "clean_adventure = text_cleaner(raw_adventure)\n",
    "clean_lore = text_cleaner(raw_lore)\n",
    "clean_mystery = text_cleaner(raw_mystery)\n",
    "clean_romance = text_cleaner(raw_romance)\n",
    "clean_science_fiction = text_cleaner(raw_science_fiction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Dan Morgan told himself he would ...]'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_adventure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "adventure_doc = nlp(clean_adventure)\n",
    "#lore_doc = nlp(raw_lore)\n",
    "#mystery_doc = nlp(raw_mystery)\n",
    "#romance_doc = nlp(raw_romance)\n",
    "#science_fiction_doc = nlp(raw_science_fiction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dan Morgan told himself he would ...]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adventure_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dan</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morgan</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>told</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>himself</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1\n",
       "0        [  adventure\n",
       "1      Dan  adventure\n",
       "2   Morgan  adventure\n",
       "3     told  adventure\n",
       "4  himself  adventure"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adventure_sents = [[sent, \"adventure\"] for sent in adventure_doc]\n",
    "#lore_sents = [[sent, \"lore\"] for sent in lore_doc]\n",
    "#mystery_sents = [[sent, \"mystery\"] for sent in mystery_doc]\n",
    "#romance_sents = [[sent, \"romance\"] for sent in romance_doc]\n",
    "#science_fiction_sents = [[sent, \"science_fiction\"] for sent in science_fiction_doc]\n",
    "\n",
    "sentences = pd.DataFrame(adventure_sents)\n",
    "\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 3000 most common words.\n",
    "# upped word cound from 2000 to 3000 because we aren't comparing works from\n",
    "# two different authors but from 5 different genres.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(3000)]\n",
    "    \n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adventure_text = brown.words(categories='adventure')\n",
    "lore_text = brown.words(categories='lore')\n",
    "mystery_text = brown.words(categories='mystery')\n",
    "romance_text = brown.words(categories='romance')\n",
    "science_fiction_text = brown.words(categories='science_fiction')\n",
    "\n",
    "common_words = set(adventure_text + lore_text + mystery_text + romance_text + science_fiction_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_punct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-62c7edc7430a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-aa0c6dbc56ad>\u001b[0m in \u001b[0;36mbow_features\u001b[0;34m(sentences, common_words)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# stop words, and uncommon words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         words = [token.lemma_\n\u001b[0;32m---> 30\u001b[0;31m                  \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                  if (\n\u001b[1;32m     32\u001b[0m                      \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_punct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-aa0c6dbc56ad>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m                  \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                  if (\n\u001b[0;32m---> 32\u001b[0;31m                      \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_punct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                      \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                      \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_punct'"
     ]
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up pipeline pieces here\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "steps = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk.tokenize in nltk:\n",
      "\n",
      "NAME\n",
      "    nltk.tokenize - NLTK Tokenizer Package\n",
      "\n",
      "DESCRIPTION\n",
      "    Tokenizers divide strings into lists of substrings.  For example,\n",
      "    tokenizers can be used to find the words and punctuation in a string:\n",
      "    \n",
      "        >>> from nltk.tokenize import word_tokenize\n",
      "        >>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
      "        ... two of them.\\n\\nThanks.'''\n",
      "        >>> word_tokenize(s)\n",
      "        ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.',\n",
      "        'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "    \n",
      "    This particular tokenizer requires the Punkt sentence tokenization\n",
      "    models to be installed. NLTK also provides a simpler,\n",
      "    regular-expression based tokenizer, which splits text on whitespace\n",
      "    and punctuation:\n",
      "    \n",
      "        >>> from nltk.tokenize import wordpunct_tokenize\n",
      "        >>> wordpunct_tokenize(s)\n",
      "        ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.',\n",
      "        'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "    \n",
      "    We can also operate at the level of sentences, using the sentence\n",
      "    tokenizer directly as follows:\n",
      "    \n",
      "        >>> from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "        >>> sent_tokenize(s)\n",
      "        ['Good muffins cost $3.88\\nin New York.', 'Please buy me\\ntwo of them.', 'Thanks.']\n",
      "        >>> [word_tokenize(t) for t in sent_tokenize(s)]\n",
      "        [['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],\n",
      "        ['Please', 'buy', 'me', 'two', 'of', 'them', '.'], ['Thanks', '.']]\n",
      "    \n",
      "    Caution: when tokenizing a Unicode string, make sure you are not\n",
      "    using an encoded version of the string (it may be necessary to\n",
      "    decode it first, e.g. with ``s.decode(\"utf8\")``.\n",
      "    \n",
      "    NLTK tokenizers can produce token-spans, represented as tuples of integers\n",
      "    having the same semantics as string slices, to support efficient comparison\n",
      "    of tokenizers.  (These methods are implemented as generators.)\n",
      "    \n",
      "        >>> from nltk.tokenize import WhitespaceTokenizer\n",
      "        >>> list(WhitespaceTokenizer().span_tokenize(s))\n",
      "        [(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36), (38, 44),\n",
      "        (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]\n",
      "    \n",
      "    There are numerous ways to tokenize text.  If you need more control over\n",
      "    tokenization, see the other methods provided in this package.\n",
      "    \n",
      "    For further information, please see Chapter 3 of the NLTK book.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    api\n",
      "    casual\n",
      "    moses\n",
      "    mwe\n",
      "    punkt\n",
      "    regexp\n",
      "    repp\n",
      "    sexpr\n",
      "    simple\n",
      "    stanford\n",
      "    stanford_segmenter\n",
      "    texttiling\n",
      "    toktok\n",
      "    treebank\n",
      "    util\n",
      "\n",
      "FUNCTIONS\n",
      "    sent_tokenize(text, language='english')\n",
      "        Return a sentence-tokenized copy of *text*,\n",
      "        using NLTK's recommended sentence tokenizer\n",
      "        (currently :class:`.PunktSentenceTokenizer`\n",
      "        for the specified language).\n",
      "        \n",
      "        :param text: text to split into sentences\n",
      "        :param language: the model name in the Punkt corpus\n",
      "    \n",
      "    word_tokenize(text, language='english', preserve_line=False)\n",
      "        Return a tokenized copy of *text*,\n",
      "        using NLTK's recommended word tokenizer\n",
      "        (currently an improved :class:`.TreebankWordTokenizer`\n",
      "        along with :class:`.PunktSentenceTokenizer`\n",
      "        for the specified language).\n",
      "        \n",
      "        :param text: text to split into words\n",
      "        :param text: str\n",
      "        :param language: the model name in the Punkt corpus\n",
      "        :type language: str\n",
      "        :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\n",
      "        :type preserver_line: bool\n",
      "\n",
      "DATA\n",
      "    improved_close_quote_regex = re.compile('([»”’])')\n",
      "    improved_open_quote_regex = re.compile('([«“‘])')\n",
      "    improved_punct_regex = re.compile('([^\\\\.])(\\\\.)([\\\\]\\\\)}>\"\\\\\\'»”’ ]*)...\n",
      "\n",
      "FILE\n",
      "    /anaconda/lib/python3.6/site-packages/nltk/tokenize/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up pipeline here\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put assesment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
