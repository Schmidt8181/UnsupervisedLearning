{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import corpus here\n",
    "# nltk.download() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I chose to only use 5 categories in my work here because I did not want overload my kernel as I am on a MacBookAir and do not have unlimited space in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_adventure = brown.sents(categories='adventure')\n",
    "raw_lore = brown.sents(categories='lore')\n",
    "raw_mystery = brown.sents(categories='mystery')\n",
    "raw_romance = brown.sents(categories='romance')\n",
    "raw_science_fiction = brown.sents(categories='science_fiction')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637\n",
      "4881\n",
      "3886\n",
      "4431\n",
      "948\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_adventure))\n",
    "print(len(raw_lore))\n",
    "print(len(raw_mystery))\n",
    "print(len(raw_romance))\n",
    "print(len(raw_science_fiction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_adventure = [\" \".join(sent) for sent in raw_adventure]\n",
    "joined_lore = [\" \".join(sent) for sent in raw_lore]\n",
    "joined_mystery = [\" \".join(sent) for sent in raw_mystery]\n",
    "joined_romance = [\" \".join(sent) for sent in raw_romance]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "table = str.maketrans({key:None for key in punctuation})\n",
    "cleaned_adventure = [sent.translate(table) for sent in joined_adventures]\n",
    "cleaned_lore = [sent.translate(table) for sent in joined_lore]\n",
    "cleaned_mystery = [sent.translate(table) for sent in joined_mystery]\n",
    "cleaned_romance = [sent.translate(table) for sent in joined_romance]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    \n",
    "    return text\n",
    "    \n",
    "clean_adventure = text_cleaner(cleaned_adventure)\n",
    "#clean_lore = text_cleaner(raw_lore)\n",
    "#clean_mystery = text_cleaner(raw_mystery)\n",
    "#clean_romance = text_cleaner(raw_romance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dan Morgan told himself he would forget Ann Turner '"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_adventure[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dan Morgan told himself he would forget Ann Tu...</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He was well rid of her</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He certainly didnt want a wife who was fickle ...</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If he had married her  hed have been asking fo...</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But all of this was rationalization</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0          1\n",
       "0  Dan Morgan told himself he would forget Ann Tu...  adventure\n",
       "1                            He was well rid of her   adventure\n",
       "2  He certainly didnt want a wife who was fickle ...  adventure\n",
       "3  If he had married her  hed have been asking fo...  adventure\n",
       "4               But all of this was rationalization   adventure"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adventure_sents = [[sent, \"adventure\"] for sent in cleaned_adventure]\n",
    "lore_sents = [[sent, \"lore\"] for sent in cleaned_lore]\n",
    "mystery_sents = [[sent, \"mystery\"] for sent in cleaned_mystery]\n",
    "romance_sents = [[sent, \"romance\"] for sent in cleaned_romance]\n",
    "\n",
    "sentences = pd.DataFrame(adventure_sents +\n",
    "                         lore_sents +\n",
    "                         mystery_sents +\n",
    "                         romance_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 3000 most common words.\n",
    "# changed word cound from 2000 to 200 because we aren't comparing works from\n",
    "# two different authors but from 5 different genres.\n",
    "\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # make text string, pull out each word\n",
    "    text = str(text)\n",
    "    allwords = text.split()\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(200)]\n",
    "    \n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        sentence = nlp(sentence)\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "adventure_words = bag_of_words(cleaned_adventure)\n",
    "print(len(adventure_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clean = bag_of_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lore_words = bag_of_words(cleaned_lore)\n",
    "mystery_words = bag_of_words(cleaned_mystery)\n",
    "romance_words = bag_of_words(cleaned_romance)\n",
    "\n",
    "common_words = set(adventure_words +\n",
    "                   lore_words +\n",
    "                   mystery_words +\n",
    "                   romance_words)\n",
    "\n",
    "len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 1000\n",
      "Processing row 2000\n",
      "Processing row 3000\n",
      "Processing row 4000\n",
      "Processing row 5000\n",
      "Processing row 6000\n",
      "Processing row 7000\n",
      "Processing row 8000\n",
      "Processing row 9000\n",
      "Processing row 10000\n",
      "Processing row 11000\n",
      "Processing row 12000\n",
      "Processing row 13000\n",
      "Processing row 14000\n",
      "Processing row 15000\n",
      "Processing row 16000\n",
      "Processing row 17000\n",
      "Processing row 18000\n",
      "Processing row 19000\n",
      "Processing row 20000\n",
      "Processing row 21000\n",
      "Processing row 22000\n"
     ]
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make tf-idf df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actually_raw_adventure = brown.raw(categories='adventure')\n",
    "actually_raw_lore = brown.raw(categories='lore')\n",
    "actually_raw_mystery = brown.raw(categories='mystery')\n",
    "actually_raw_romance = brown.raw(categories='romance')\n",
    "\n",
    "combined = actually_raw_adventure + actually_raw_lore + actually_raw_mystery + actually_raw_romance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "actually_raw_adventure = actually_raw_adventure.split('\\n\\n\\n\\t')\n",
    "actually_raw_lore = actually_raw_lore.split('\\n\\n\\n\\t')\n",
    "actually_raw_mystery = actually_raw_mystery.split('\\n\\n\\n\\t')\n",
    "actually_raw_romance = actually_raw_romance.split('\\n\\n\\n\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure length:  1307\n",
      "lore length:  1042\n",
      "mystery length:  1080\n",
      "romance length:  1154\n"
     ]
    }
   ],
   "source": [
    "# join separeted paragraphs here\n",
    "print(\"adventure length: \", len(actually_raw_adventure))\n",
    "print(\"lore length: \", len(actually_raw_lore))\n",
    "print(\"mystery length: \", len(actually_raw_mystery))\n",
    "print(\"romance length: \", len(actually_raw_romance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure length:  1307\n",
      "adventure + lore length:  2349\n",
      "with mystery length:  3429\n",
      "with romance length:  4583\n"
     ]
    }
   ],
   "source": [
    "print(\"adventure length: \", len(actually_raw_adventure))\n",
    "combined = actually_raw_adventure + actually_raw_lore\n",
    "print(\"adventure + lore length: \", len(combined))\n",
    "combined = combined + actually_raw_mystery\n",
    "print(\"with mystery length: \", len(combined))\n",
    "combined = combined + actually_raw_romance\n",
    "print(\"with romance length: \", len(combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\tDan/np Morgan/np told/vbd himself/ppl he/pps would/md forget/vb Ann/np Turner/np ./.\\nHe/pps was/bedz well/rb rid/jj of/in her/ppo ./.\\nHe/pps certainly/rb didn't/dod* want/vb a/at wife/nn who/wps was/bedz fickle/jj as/cs Ann/np ./.\\nIf/cs he/pps had/hvd married/vbn her/ppo ,/, he'd/pps+md have/hv been/ben asking/vbg for/in trouble/nn ./.\""
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actually_raw_adventure[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4583"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\n\\tDan/np Morgan/np told/vbd himself/ppl he/pps would/md forget/vb Ann/np Turner/np ./.\\nHe/pps was/bedz well/rb rid/jj of/in her/ppo ./.\\nHe/pps certainly/rb didn't/dod* want/vb a/at wife/nn who/wps was/bedz fickle/jj as/cs Ann/np ./.\\nIf/cs he/pps had/hvd married/vbn her/ppo ,/, he'd/pps+md have/hv been/ben asking/vbg for/in trouble/nn ./.\", \"But/cc all/abn of/in this/dt was/bedz rationalization/nn ./.\\nSometimes/rb he/pps woke/vbd up/rp in/in the/at middle/nn of/in the/at night/nn thinking/vbg of/in Ann/np ,/, and/cc then/rb could/md not/* get/vb back/rb to/in sleep/nn ./.\\nHis/pp$ plans/nns and/cc dreams/nns had/hvd revolved/vbn around/in her/ppo so/ql much/rb and/cc for/in so/ql long/jj that/cs now/rb he/pps felt/vbd as/cs if/cs he/pps had/hvd nothing/pn ./.\\nThe/at easiest/jjt thing/nn would/md be/be to/to sell/vb out/rp to/in Al/np Budd/np and/cc leave/vb the/at country/nn ,/, but/cc there/ex was/bedz a/at stubborn/jj streak/nn in/in him/ppo that/dt wouldn't/md* allow/vb it/ppo ./.\", 'The/at best/jjt antidote/nn for/in the/at bitterness/nn and/cc disappointment/nn that/wps poisoned/vbd him/ppo was/bedz hard/jj work/nn ./.\\nHe/pps found/vbd that/cs if/cs he/pps was/bedz tired/vbn enough/qlp at/in night/nn ,/, he/pps went/vbd to/in sleep/nn simply/rb because/cs he/pps was/bedz too/ql exhausted/vbn to/to stay/vb awake/rb ./.\\nEach/dt day/nn he/pps found/vbd himself/ppl thinking/vbg less/ql often/rb of/in Ann/np ;/. ;/.\\neach/dt day/nn the/at hurt/nn was/bedz a/at little/ap duller/jjr ,/, a/at little/ap less/ql poignant/jj ./.', 'He/pps had/hvd plenty/nn of/in work/nn to/to do/do ./.\\nBecause/cs the/at summer/nn was/bedz unusually/rb dry/jj and/cc hot/jj ,/, the/at spring/nn produced/vbd a/at smaller/jjr stream/nn than/cs in/in ordinary/jj years/nns ./.\\nThe/at grass/nn in/in the/at meadows/nns came/vbd fast/rb ,/, now/rb that/cs the/at warm/jj weather/nn was/bedz here/rb ./.\\nHe/pps could/md not/* afford/vb to/to lose/vb a/at drop/nn of/in the/at precious/jj water/nn ,/, so/cs he/pps spent/vbd most/ap of/in his/pp$ waking/vbg hours/nns along/in the/at ditches/nns in/in his/pp$ meadows/nns ./.']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\tDan/np Morgan/np told/vbd himself/ppl he/pps would/md forget/vb Ann/np Turner/np ./.\\nHe/pps was/bedz well/rb rid/jj of/in her/ppo ./.\\nHe/pps certainly/rb didn't/dod* want/vb a/at wife/nn who/wps was/bedz fickle/jj as/cs Ann/np ./.\\nIf/cs he/pps had/hvd married/vbn her/ppo ,/, he'd/pps+md have/hv been/ben asking/vbg for/in trouble/nn ./.\""
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = str(combined)\n",
    "combined = combined.split(',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"\\\\n\\\\n\\\\tDan/np Morgan/np told/vbd himself/ppl he/pps would/md forget/vb Ann/np Turner/np ./.\\\\nHe/pps was/bedz well/rb rid/jj of/in her/ppo ./.\\\\nHe/pps certainly/rb didn\\'t/dod* want/vb a/at wife/nn who/wps was/bedz fickle/jj as/cs Ann/np ./.\\\\nIf/cs he/pps had/hvd married/vbn her/ppo '"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\tDan/np Morgan/np told/vbd himself/ppl he/pps would/md forget/vb Ann/np Turner/np ./.\\nHe/pps was/bedz well/rb rid/jj of/in her/ppo ./.\\nHe/pps certainly/rb didn't/dod* want/vb a/at wife/nn who/wps was/bedz fickle/jj as/cs Ann/np ./.\\nIf/cs he/pps had/hvd married/vbn her/ppo ,/, he'd/pps+md have/hv been/ben asking/vbg for/in trouble/nn ./.\""
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the combined list of paragraphs into a string made it so when i resplit the list\n",
    "# with .split(',') the paragraphs were not the same size as originally\n",
    "# now i'm combining the raw categories before splitting them into paragraphs to try again\n",
    "adventure = brown.raw(categories='adventure')\n",
    "lore = brown.raw(categories='lore')\n",
    "mystery = brown.raw(categories='mystery')\n",
    "romance = brown.raw(categories='romance')\n",
    "\n",
    "combined2 = adventure + lore + mystery + romance\n",
    "combined2 = combined2.split('\\n\\n\\n\\t')\n",
    "combined2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\tDan/np Morgan/np told/vbd himself/ppl he/pps would/md forget/vb Ann/np Turner/np ./.\\nHe/pps was/bedz well/rb rid/jj of/in her/ppo ./.\\nHe/pps certainly/rb didn't/dod* want/vb a/at wife/nn who/wps was/bedz fickle/jj as/cs Ann/np ./.\\nIf/cs he/pps had/hvd married/vbn her/ppo ,/, he'd/pps+md have/hv been/ben asking/vbg for/in trouble/nn ./.\""
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actually_raw_adventure[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length combined2:  4579\n",
      "length adventure:  1303\n",
      "length lore:  1042\n",
      "length mystery:  1080\n",
      "length romance:  1154\n",
      "adventure indicies should end at: 1303\n",
      "lore indicies should end at:  2345\n",
      "mystery indicies should end at:  3425\n",
      "romance indicies should end at: 4579\n"
     ]
    }
   ],
   "source": [
    "print('length combined2: ', len(combined2))\n",
    "print('length adventure: ', len(adventure.split('\\n\\n\\n\\t')))\n",
    "print('length lore: ', len(lore.split('\\n\\n\\n\\t')))\n",
    "print('length mystery: ', len(mystery.split('\\n\\n\\n\\t')))\n",
    "print('length romance: ', len(romance.split('\\n\\n\\n\\t')))\n",
    "\n",
    "print('adventure indicies should end at:', len(adventure.split('\\n\\n\\n\\t')))\n",
    "print('lore indicies should end at: ', len((adventure + lore).split('\\n\\n\\n\\t')))\n",
    "print('mystery indicies should end at: ', len((adventure + lore + mystery).split('\\n\\n\\n\\t')))\n",
    "print('romance indicies should end at:', len((adventure + lore + mystery + romance).split('\\n\\n\\n\\t')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 11652\n",
      "Original sentence: His/pp$ wide/jj mouth/nn compressed/vbd ./.\n",
      "In/in a/at way/nn ,/, he/pps couldn't/md* blame/vb her/ppo ./.\n",
      "He/pps had/hvd picked/vbn out/rp this/dt pathless/jj trail/nn ,/, instead/rb of/in the/at common/jj one/pn ,/, in/in a/at moment/nn of/in romantic/jj fancy/nn ,/, to/to give/vb them/ppo privacy/nn on/in their/pp$ honeymoon/nn ./.\n",
      "Tf_idf vector: {'Rourke': 0.26470756380021243, 'Turn': 0.3309259034569111, 'yes': 0.24942003913473113, 'block': 0.2539129094404099, 'number': 0.2201980231343145, 'side': 0.18737929032726755, 'me': 0.13983816302168256, 'right': 0.16422020464324277, 'Not': 0.203283864413083, 'gave': 0.1997424307017251, 'better': 0.18801228385465538, 'you': 0.10971910456328318, 'hand': 0.17641586551449281, 'see': 0.15612680198525877, 'said': 0.11151776001878064, 'think': 0.1794094594878007, 'left': 0.17453467990730198, 'more': 0.1427050574923614, 'rbr': 0.14555883562445493, 'ppss': 0.13674271095634935, 'or': 0.1249068636787584, 'nr': 0.1422830664316856, 'than': 0.14949162277714884, 'ap': 0.09070970058051647, 'it': 0.08952817595214597, 'out': 0.11664426091499404, 'that': 0.08540343310014972, 'so': 0.13446799364146386, 'to': 0.06204145449549346, 'could': 0.13153688303889172, 'and': 0.061758780598560964, 'the': 0.10627219799617804, 'dt': 0.08558823057101159, 'cc': 0.11210074053301332, 'for': 0.09261086859108077, 'ppo': 0.13250102082732942, 'vb': 0.16626838671534067, 'md': 0.0764443234292975, 'pps': 0.05956469617801859, 'he': 0.08984972746100198, 'np': 0.05795208955929854}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(combined2, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.75, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice, \n",
    "                             lowercase=False, #convert everything to lower case \n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies. Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "brown_paras_tfidf=vectorizer.fit_transform(combined2)\n",
    "print(\"Number of features: %d\" % brown_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(brown_paras_tfidf, test_size=0.4)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4579, 11652)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_paras_tfidf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df = pd.DataFrame(data=brown_paras_tfidf.toarray(),\n",
    "                             index=np.arange(brown_paras_tfidf.shape[0]),\n",
    "                             columns=(feature_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df['categories'] = None\n",
    "vectorized_df.loc[0:1303, 'categories'] = 'adventure'\n",
    "vectorized_df.loc[1303:2345, 'categories'] = 'lore'\n",
    "vectorized_df.loc[2345:3425, 'categories'] = 'mystery'\n",
    "vectorized_df.loc[3425:4579, 'categories'] = 'romance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = (vectorizer.vocabulary_)\n",
    "feature_sorted = sorted(feature_names, key = feature_names.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "#(change this line) lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.888\n",
      "\n",
      "Test set score: 0.29431741000679196\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13250, 304) (13250,)\n",
      "Training set score: 0.3772075471698113\n",
      "\n",
      "Test set score: 0.34480416572334166\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(nltk.tokenize)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
